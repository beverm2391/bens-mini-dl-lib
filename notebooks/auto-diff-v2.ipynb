{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=True, parents=None, creation_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.parents = parents or []\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        if self.requires_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None and self.grad is None:\n",
    "            # if self is a leaf node, we can start from 1\n",
    "            grad = np.ones_like(self.data)\n",
    "\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "\n",
    "        if self.creation_op == 'add':\n",
    "            self.parents[0].backward(self.grad)\n",
    "            self.parents[1].backward(self.grad)\n",
    "        elif self.creation_op == 'mul':\n",
    "            self.parents[0].backward(self.grad * self.parents[1].data)\n",
    "            self.parents[1].backward(self.grad * self.parents[0].data)\n",
    "        # TODO: add more ops\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data, requires_grad=True, parents=[self, other], creation_op='add')\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "    \n",
    "    # TODO: add more ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating my tensor class to handle automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=True, parents=None, creation_op=None):\n",
    "        self.data = np.array(data) # data\n",
    "        self.shape = self.data.shape # shape of data\n",
    "        self.requires_grad = requires_grad # whether to calculate gradients\n",
    "        self.parents = parents or []\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = self.zero_grad() if requires_grad else None # gradient of data, if needed\n",
    "        self.is_scalar = self.data.ndim == 0 # whether the data is a scalar\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "    \n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad: # if this tensor doesn't require gradients, return\n",
    "            return\n",
    "        if grad is None and self.grad is None:\n",
    "            # if self is a leaf node, we can start from 1\n",
    "            grad = Tensor(np.ones_like(self.data))\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad # if self is a leaf node, we accumulate gradients\n",
    "\n",
    "        # time to backpropogate\n",
    "        if self.creation_op == 'add':\n",
    "            self.parents[0].backward(self.grad)\n",
    "            self.parents[1].backward(self.grad)\n",
    "        elif self.creation_op == 'sub':\n",
    "            self.parents[0].backward(self.grad)\n",
    "            self.parents[1].backward(-self.grad)\n",
    "        elif self.creation_op == 'mul':\n",
    "            self.parents[0].backward(self.grad * self.parents[1])\n",
    "            self.parents[1].backward(self.grad * self.parents[0])\n",
    "        elif self.creation_op == 'div':\n",
    "            self.parents[0].backward(self.grad / self.parents[1])\n",
    "            self.parents[1].backward(-self.grad * self.parents[0] / self.parents[1] ** 2)\n",
    "        elif self.creation_op == 'pow':\n",
    "            self.parents[0].backward(self.grad * self.parents[1].data * (self.parents[0].data ** (self.parents[1].data - 1)))\n",
    "            self.parents[1].backward(self.grad * (self.parents[0].data ** self.parents[1].data) * np.log(self.parents[0].data))\n",
    "        elif self.creation_op == 'matmul':\n",
    "            self.parents[0].backward(grad @ self.parents[1].data.T)\n",
    "            self.parents[1].backward(self.parents[0].data.T @ grad)\n",
    "        elif self.creation_op == 'neg':\n",
    "            self.parents[0].backward(-self.grad)\n",
    "        elif self.creation_op == 'transpose':\n",
    "            self.parents[0].backward(self.grad.T)\n",
    "        elif self.creation_op == 'abs':\n",
    "            self.parents[0].backward(self.grad * np.sign(self.parents[0].data))\n",
    "        elif self.creation_op == 'sum':\n",
    "            self.parents[0].backward(self.grad * np.ones_like(self.parents[0].data))\n",
    "        elif self.creation_op == 'mean':\n",
    "            self.parents[0].backward(self.grad * np.ones_like(self.parents[0].data) / np.size(self.parents[0].data))\n",
    "        elif self.creation_op == 'max':\n",
    "            self.parents[0].backward(self.grad * (self.parents[0].data == self.data))\n",
    "        elif self.creation_op == 'min':\n",
    "            self.parents[0].backward(self.grad * (self.parents[0].data == self.data))\n",
    "        elif self.creation_op == 'std':\n",
    "            mean_val = np.mean(self.parents[0].data)\n",
    "            N = self.parents[0].data.size\n",
    "            self.parents[0].backward(self.grad * (self.parents[0].data - mean_val) / (self.data * np.sqrt(N)))\n",
    "        elif self.creation_op == 'reshape':\n",
    "            self.parents[0].backward(self.grad.reshape(self.parents[0].shape)) # reshape to the shape of the parent\n",
    "        elif self.creation_op == 'squeeze':\n",
    "            axis = self.meta_info['axis']\n",
    "            if axis is not None:\n",
    "                grad_expanded = np.expand_dims(grad, axis=axis)\n",
    "            else:\n",
    "                grad_expanded = grad\n",
    "            self.parents[0].backward(grad_expanded)\n",
    "        elif self.creation_op == 'unsqueeze':\n",
    "            axis = self.meta_info['axis']\n",
    "            self.parents[0].backward(np.squeeze(grad, axis=axis))\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Gradient for {self.creation_op} not implemented\")\n",
    "        # TODO: add broadcasting, indexing\n",
    "\n",
    "    # Basic Operations ============================================\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data + other, requires_grad=True, parents=[self, other], creation_op='add')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data + other.data, requires_grad=True, parents=[self, other], creation_op='add')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for addition: {type(other)}\")\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data - other, requires_grad=True, parents=[self, other], creation_op='sub')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data - other.data, requires_grad=True, parents=[self, other], creation_op='sub')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for subtraction: {type(other)}\")\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data * other, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for multiplication: {type(other)}\")\n",
    "        \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data / other, requires_grad=True, parents=[self, other], creation_op='div')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data / other.data, requires_grad=True, parents=[self, other], creation_op='div')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for division: {type(other)}\")\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data ** other, requires_grad=True, parents=[self, other], creation_op='pow')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data ** other.data, requires_grad=True, parents=[self, other], creation_op='pow')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for power: {type(other)}\")\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        if self.is_scalar:\n",
    "            return Tensor(self.data * other.data)\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        \n",
    "        if self.data.ndim == 0 and other.data.ndim == 0: # if both are scalars\n",
    "            warnings.warn(\"Both of your inputs are scalars. Using element-wise multiplication instead. Use the * operator insead of @.\")\n",
    "            return self.data * other.data\n",
    "        \n",
    "        if self.data.ndim == 0 and other.data.ndim > 0: # if self is a scalar and other is not\n",
    "            warnings.warn(\"One of your inputs is a scalar. Using element-wise multiplication instead. Use the * operator insead of @.\")\n",
    "            return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        \n",
    "        if self.data.ndim > 0 and other.data.ndim == 0: # if self is not a scalar and other is\n",
    "            warnings.warn(\"One of your inputs is a scalar. Using element-wise multiplication instead. Use the * operator insead of @.\")\n",
    "            return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        \n",
    "        # v * v\n",
    "        if self.data.ndim == 1 and other.data.ndim == 1: # if both are vectors\n",
    "            if self.data.shape[0] != other.data.shape[0]: # if the vectors are not the same length\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "\n",
    "        # v * m\n",
    "        if self.data.ndim == 1 and other.data.ndim > 1:\n",
    "            if self.data.shape[0] != other.data.shape[-2]:\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "            \n",
    "        # m * v\n",
    "        if self.data.ndim > 1 and other.data.ndim == 1:\n",
    "            if self.data.shape[-1] != other.data.shape[0]:\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "\n",
    "        #  m * m\n",
    "        if self.data.ndim > 1 and other.data.ndim > 1:\n",
    "            if self.data.shape[-1] != other.data.shape[-2]:\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "\n",
    "        result = np.matmul(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=True, parents=[self, other], creation_op='matmul')\n",
    "    \n",
    "    # Reverse Operations ============================================\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Tensor(other - self.data)  # Note the order\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return Tensor(other / self.data)  # Note the order\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        return Tensor(other ** self.data)\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        # The other array should be on the left-hand side now\n",
    "        # assuming self.data and other.data are NumPy arrays\n",
    "        result = np.matmul(other.data, self.data)\n",
    "        return Tensor(result)\n",
    "    \n",
    "    # Unary Operations ============================================\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __abs__(self):\n",
    "        return Tensor(np.abs(self.data), requires_grad=self.requires_grad, parents=[self], creation_op='abs')\n",
    "    \n",
    "    # Reduction Operations ========================================\n",
    "    def sum(self, axis=None):\n",
    "        if self.is_scalar:\n",
    "            return self # can't sum a scalar\n",
    "        return Tensor(self.data.sum(axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='sum')\n",
    "\n",
    "    def mean(self, axis=None):\n",
    "        if self.is_scalar:\n",
    "            return self # can't mean a scalar\n",
    "        return Tensor(self.data.mean(axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='mean')\n",
    "        \n",
    "    def max(self, axis=None):\n",
    "        if self.is_scalar:\n",
    "            return self # can't max a scalar\n",
    "        return Tensor(self.data.max(axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='max')\n",
    "        \n",
    "    def min(self, axis=None):\n",
    "        if self.is_scalar:\n",
    "            return self # can't min a scalar\n",
    "        return Tensor(self.data.min(axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='min')\n",
    "    \n",
    "    def std(self, axis=None):\n",
    "        if self.is_scalar:\n",
    "            return self # can't std a scalar\n",
    "        return Tensor(self.data.std(axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='std')\n",
    "    \n",
    "    # Shape Operations ==================================================\n",
    "    def reshape(self, *new_shape):\n",
    "        return Tensor(self.data.reshape(new_shape), requires_grad=self.requires_grad, parents=[self], creation_op='reshape', \\\n",
    "                    meta_info = {\"original_shape\" : self.shape})\n",
    "    \n",
    "    def squeeze(self, axis=None):\n",
    "        return Tensor(self.data.squeeze(axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='squeeze', \\\n",
    "                        meta_info = {\"axis\" : axis} if axis is not None else None)\n",
    "    \n",
    "    def unsqueeze(self, axis):\n",
    "        return Tensor(np.expand_dims(self.data, axis=axis), requires_grad=self.requires_grad, parents=[self], creation_op='unsqueeze', \\\n",
    "                        meta_info = {\"axis\" : axis})\n",
    "\n",
    "    def transpose(self, axes=None):\n",
    "        return Tensor(np.transpose(self.data, axes), requires_grad=self.requires_grad, parents=[self], creation_op='transpose')\n",
    "    \n",
    "    # Comparison Operations =======================================\n",
    "    def __gt__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data > other)\n",
    "        elif isinstance(other, Tensor):\n",
    "            if self.data.shape != other.data.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            return Tensor(self.data > other.data)\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for comparison\")\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data < other)\n",
    "        elif isinstance(other, Tensor):\n",
    "            if self.data.shape != other.data.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            return Tensor(self.data < other.data)\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for comparison\")\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data >= other)\n",
    "        elif isinstance(other, Tensor):\n",
    "            if self.data.shape != other.data.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            return Tensor(self.data >= other.data)\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for comparison\")\n",
    "\n",
    "    def __le__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data <= other)\n",
    "        elif isinstance(other, Tensor):\n",
    "            if self.data.shape != other.data.shape:\n",
    "                raise ValueError(\"Shape mismatch\")\n",
    "            return Tensor(self.data <= other.data)\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for comparison\")\n",
    "\n",
    "    # Indexing Operations ================================================\n",
    "    def __getitem__(self, index):\n",
    "        return Tensor(self.data[index])\n",
    "    \n",
    "    def __setitem__(self, index, value):\n",
    "        self.data[index] = value if isinstance(value, Tensor) else Tensor(value)\n",
    "\n",
    "    # Utility Methods ====================================================\n",
    "    def _broadcast_tensors(self, other):\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        a_shape, b_shape = np.broadcast_arrays(self.data, other.data).shape\n",
    "        return Tensor(self.data * np.ones(a_shape)), Tensor(other.data * np.ones(b_shape))\n",
    "\n",
    "    def clone(self):\n",
    "        return Tensor(self.data.copy(), self.requires_grad)\n",
    "    \n",
    "    def detach(self):\n",
    "        return Tensor(self.data, requires_grad=False)\n",
    "    \n",
    "    def to(self, device):\n",
    "        # TODO: Implement device transfer\n",
    "        # Here is where device transfer would go. NumPy arrays are always on the CPU, so this is a placeholder\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    # Other Methods =======================================================\n",
    "    @property\n",
    "    def T(self, axes=None):\n",
    "        return self.transpose(axes=axes)\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        s = 1\n",
    "        for dim in self.shape:\n",
    "            s *= dim\n",
    "        return s\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Tensor):\n",
    "            return False\n",
    "        return np.array_equal(self.data, other.data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.data}, requires_grad={self.requires_grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tensor([1, 2, 3, 4, 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
