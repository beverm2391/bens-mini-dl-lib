{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=True, parents=None, creation_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.parents = parents or []\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        if self.requires_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        if grad is None and self.grad is None:\n",
    "            # if self is a leaf node, we can start from 1\n",
    "            grad = np.ones_like(self.data)\n",
    "\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "\n",
    "        if self.creation_op == 'add':\n",
    "            self.parents[0].backward(self.grad)\n",
    "            self.parents[1].backward(self.grad)\n",
    "        elif self.creation_op == 'mul':\n",
    "            self.parents[0].backward(self.grad * self.parents[1].data)\n",
    "            self.parents[1].backward(self.grad * self.parents[0].data)\n",
    "        # TODO: add more ops\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data, requires_grad=True, parents=[self, other], creation_op='add')\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "    \n",
    "    # TODO: add more ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating my tensor class to handle automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=True, parents=None, creation_op=None):\n",
    "        self.data = np.array(data) # data\n",
    "        self.shape = self.data.shape # shape of data\n",
    "        self.requires_grad = requires_grad # whether to calculate gradients\n",
    "        self.parents = parents or []\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = self.zero_grad() if requires_grad else None # gradient of data, if needed\n",
    "        self.is_scalar = self.data.ndim == 0 # whether the data is a scalar\n",
    "\n",
    "    def zero_grad(self):\n",
    "        return Tensor(np.zeros_like(self.data))\n",
    "    \n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad: # if this tensor doesn't require gradients, return\n",
    "            return\n",
    "        if grad is None and self.grad is None:\n",
    "            # if self is a leaf node, we can start from 1\n",
    "            grad = Tensor(np.ones_like(self.data))\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad # if self is a leaf node, we accumulate gradients\n",
    "\n",
    "        # time to backpropogate\n",
    "        if self.creation_op == 'add':\n",
    "            self.parents[0].backward(self.grad)\n",
    "            self.parents[1].backward(self.grad)\n",
    "        elif self.creation_op == 'sub':\n",
    "            self.parents[0].backward(self.grad)\n",
    "            self.parents[1].backward(-self.grad)\n",
    "        elif self.creation_op == 'mul':\n",
    "            self.parents[0].backward(self.grad * self.parents[1])\n",
    "            self.parents[1].backward(self.grad * self.parents[0])\n",
    "        elif self.creation_op == 'div':\n",
    "            self.parents[0].backward(self.grad / self.parents[1])\n",
    "            self.parents[1].backward(-self.grad * self.parents[0] / self.parents[1] ** 2)\n",
    "        elif self.creation_op == 'pow':\n",
    "            self.parents[0].backward(self.grad * self.parents[1].data * (self.parents[0].data ** (self.parents[1].data - 1)))\n",
    "            self.parents[1].backward(self.grad * (self.parents[0].data ** self.parents[1].data) * np.log(self.parents[0].data))\n",
    "        elif self.creation_op == 'matmul':\n",
    "            self.parents[0].backward(grad @ self.parents[1].data.T)\n",
    "            self.parents[1].backward(self.parents[0].data.T @ grad)\n",
    "        elif self.creation_op == 'neg':\n",
    "            self.parents[0].backward(-self.grad)\n",
    "        elif self.creation_op == 'transpose':\n",
    "            self.parents[0].backward(self.grad.T)\n",
    "        elif self.creation_op == 'abs':\n",
    "            self.parents[0].backward(self.grad * np.sign(self.parents[0].data))\n",
    "\n",
    "\n",
    "    # Basic Operations ============================================\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data + other, requires_grad=True, parents=[self, other], creation_op='add')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data + other.data, requires_grad=True, parents=[self, other], creation_op='add')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for addition: {type(other)}\")\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data - other, requires_grad=True, parents=[self, other], creation_op='sub')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data - other.data, requires_grad=True, parents=[self, other], creation_op='sub')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for subtraction: {type(other)}\")\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data * other, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for multiplication: {type(other)}\")\n",
    "        \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data / other, requires_grad=True, parents=[self, other], creation_op='div')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data / other.data, requires_grad=True, parents=[self, other], creation_op='div')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for division: {type(other)}\")\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            return Tensor(self.data ** other, requires_grad=True, parents=[self, other], creation_op='pow')\n",
    "        elif isinstance(other, Tensor):\n",
    "            return Tensor(self.data ** other.data, requires_grad=True, parents=[self, other], creation_op='pow')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type for power: {type(other)}\")\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        if self.is_scalar:\n",
    "            return Tensor(self.data * other.data)\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        \n",
    "        if self.data.ndim == 0 and other.data.ndim == 0: # if both are scalars\n",
    "            warnings.warn(\"Both of your inputs are scalars. Using element-wise multiplication instead. Use the * operator insead of @.\")\n",
    "            return self.data * other.data\n",
    "        \n",
    "        if self.data.ndim == 0 and other.data.ndim > 0: # if self is a scalar and other is not\n",
    "            warnings.warn(\"One of your inputs is a scalar. Using element-wise multiplication instead. Use the * operator insead of @.\")\n",
    "            return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        \n",
    "        if self.data.ndim > 0 and other.data.ndim == 0: # if self is not a scalar and other is\n",
    "            warnings.warn(\"One of your inputs is a scalar. Using element-wise multiplication instead. Use the * operator insead of @.\")\n",
    "            return Tensor(self.data * other.data, requires_grad=True, parents=[self, other], creation_op='mul')\n",
    "        \n",
    "        # v * v\n",
    "        if self.data.ndim == 1 and other.data.ndim == 1: # if both are vectors\n",
    "            if self.data.shape[0] != other.data.shape[0]: # if the vectors are not the same length\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "\n",
    "        # v * m\n",
    "        if self.data.ndim == 1 and other.data.ndim > 1:\n",
    "            if self.data.shape[0] != other.data.shape[-2]:\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "            \n",
    "        # m * v\n",
    "        if self.data.ndim > 1 and other.data.ndim == 1:\n",
    "            if self.data.shape[-1] != other.data.shape[0]:\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "\n",
    "        #  m * m\n",
    "        if self.data.ndim > 1 and other.data.ndim > 1:\n",
    "            if self.data.shape[-1] != other.data.shape[-2]:\n",
    "                raise ValueError(f\"Cannot perform matrix multiplication on tensors with shapes {self.data.shape} and {other.data.shape}.\")\n",
    "\n",
    "        result = np.matmul(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=True, parents=[self, other], creation_op='matmul')\n",
    "    \n",
    "    # Reverse Operations ============================================\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Tensor(other - self.data)  # Note the order\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return Tensor(other / self.data)  # Note the order\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        return Tensor(other ** self.data)\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "        # The other array should be on the left-hand side now\n",
    "        # assuming self.data and other.data are NumPy arrays\n",
    "        result = np.matmul(other.data, self.data)\n",
    "        return Tensor(result)\n",
    "    \n",
    "    # Unary Operations ============================================\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __pos__(self):\n",
    "        return self\n",
    "    \n",
    "    def __abs__(self):\n",
    "        return Tensor(np.abs(self.data), requires_grad=self.requires_grad, parents=[self], creation_op='abs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
