{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "from lib.Tensor import Tensor\n",
    "from lib.NN import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights = Tensor.randn(output_dim, input_dim) * 0.01 # init backwards (output_dim, input_dim) for computational efficiency\n",
    "        self.biases = Tensor.zeros(output_dim)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return inputs @ self.weights.T + self.biases # transpose weights for computational efficiency\n",
    "    \n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return [self.weights, self.biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dense():\n",
    "    input_dim = 5\n",
    "    output_dim = 3\n",
    "    batch_size = 10\n",
    "\n",
    "    pt_dense = torch.nn.Linear(input_dim, output_dim)\n",
    "    dense = Dense(input_dim, output_dim)\n",
    "\n",
    "    # copy the weights and biases from custom layer to pytorch layer\n",
    "    with torch.no_grad():\n",
    "        pt_dense.weight.copy_(torch.tensor(dense.weights.data))\n",
    "        pt_dense.bias.copy_(torch.tensor(dense.biases.data))\n",
    "\n",
    "    pt_input = torch.randn(batch_size, input_dim) # pytorch tensor\n",
    "    input = Tensor(pt_input.numpy()) # copy the pytorch tensor\n",
    "\n",
    "    pt_output = pt_dense(pt_input) # pytorch output\n",
    "    output = dense(input) # custom output\n",
    "\n",
    "    # compare the outputs\n",
    "    assert np.allclose(pt_output.detach().numpy(), output.data, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.gamma = Tensor.ones(num_features)\n",
    "        self.beta = Tensor.zeros(num_features)\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        mean = np.mean(inputs.data, axis=0)\n",
    "        var = np.var(inputs.data, axis=0)\n",
    "        normalized = (inputs.data - mean) / np.sqrt(var + self.eps)\n",
    "        out = self.gamma.data * normalized + self.beta.data\n",
    "        return Tensor(out)\n",
    "\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        mask = np.random.binomial(1, 1 - self.p, size=inputs.data.shape)\n",
    "        out = inputs.data * mask / (1 - self.p)\n",
    "        return Tensor(out)\n",
    "\n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return []  # Dropout has no learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batchnorm():\n",
    "    num_features = 4\n",
    "    batch_size = 10\n",
    "\n",
    "    # Create instances of custom and PyTorch BatchNorm layers\n",
    "    custom_bn = BatchNorm(num_features)\n",
    "    pt_bn = torch.nn.BatchNorm1d(num_features, affine=True, eps=1e-5, momentum=0)\n",
    "\n",
    "    # Initialize both layers with the same gamma and beta\n",
    "    with torch.no_grad():\n",
    "        pt_bn.weight.copy_(torch.tensor(custom_bn.gamma.data))\n",
    "        pt_bn.bias.copy_(torch.tensor(custom_bn.beta.data))\n",
    "\n",
    "    # Generate random input\n",
    "    pt_input = torch.randn(batch_size, num_features)\n",
    "    custom_input = Tensor(pt_input.numpy())\n",
    "\n",
    "    # Forward pass\n",
    "    pt_output = pt_bn(pt_input)\n",
    "    custom_output = custom_bn(custom_input)\n",
    "\n",
    "    # Check if the outputs are close\n",
    "    assert np.allclose(pt_output.detach().numpy(), custom_output.data, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dropout():\n",
    "    p = 0.5\n",
    "    shape = (10, 4)\n",
    "\n",
    "    # Create custom Dropout layer\n",
    "    custom_dropout = Dropout(p=p)\n",
    "\n",
    "    # Generate random input\n",
    "    custom_input = Tensor(np.random.randn(*shape))\n",
    "\n",
    "    # Forward pass\n",
    "    custom_output = custom_dropout(custom_input)\n",
    "\n",
    "    # Check if the Dropout layer has zeroed approximately p fraction of input\n",
    "    zero_fraction = np.mean(custom_output.data == 0)\n",
    "    assert np.isclose(zero_fraction, p, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dropout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
