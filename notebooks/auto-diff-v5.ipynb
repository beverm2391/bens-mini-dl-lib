{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from functools import wraps\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data: Union[int, float, np.ndarray], dtype: str = 'float', requires_grad: bool = False, parents=None, creation_op=None):\n",
    "        self.data = np.array(data, dtype=dtype) # The data contained in the tensor\n",
    "        self.dtype = dtype # The data type of the tensor\n",
    "\n",
    "        self.shape = self.data.shape # The shape of the tensor\n",
    "        self.ndim = self.data.ndim # The number of dimensions of the tensor\n",
    "        self.size = self.data.size # The number of elements in the tensor\n",
    "\n",
    "        self.requires_grad = requires_grad # Whether or not to compute gradients for this tensor\n",
    "        self.grad = None # The gradient of this tensor\n",
    "\n",
    "        if self.requires_grad: # If we need to compute gradients for this tensor\n",
    "            self.zero_grad() # Initialize the gradient to 0\n",
    "\n",
    "        self.parents = parents or [] # Tensors from which this one was created\n",
    "        self.creation_op = creation_op # The operation that created this tensor\n",
    "\n",
    "    @property\n",
    "    def backward_ops(self):\n",
    "        \"\"\"\n",
    "        I did this to clearly see what's implemented and what's not\n",
    "        \"\"\"\n",
    "        ops = {\n",
    "            \"add\": self.backward_add,\n",
    "            \"sub\": self.backward_sub,\n",
    "            \"mul\": self.backward_mul,\n",
    "            \"div\": self.backward_div,\n",
    "            \"pow\": self.backward_pow,\n",
    "            \"matmul\": self.backward_matmul,\n",
    "            \"transpose\": self.backward_transpose,\n",
    "        }\n",
    "        return ops\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Zero out the gradient\n",
    "        \"\"\"\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        \"\"\"\n",
    "        This function will be called recursively to backpropogate gradients (auto differentiation)\n",
    "        \"\"\"\n",
    "        if not self.requires_grad: # if this tensor doesn't require gradients, return\n",
    "            return\n",
    "        if grad is None and self.grad is None:\n",
    "            # if self is a leaf node, we can start from 1\n",
    "            grad = Tensor(np.ones_like(self.data))\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad # if self is a leaf node, we accumulate gradients\n",
    "    \n",
    "        # time to backpropogate\n",
    "        backward_op = self.backward_ops.get(self.creation_op) # get the correct backward op\n",
    "        if backward_op:\n",
    "            backward_op() # call the backward op\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Backward op for {self.creation_op} not implemented\")\n",
    "        \n",
    "    def make_tensor(func):\n",
    "        \"\"\"\n",
    "        Decorator to convert the 'other' arg to a tensor if its not already a tensor\n",
    "        \"\"\"\n",
    "        def wrapper(self, other):\n",
    "            if not isinstance(other, Tensor):\n",
    "                other = Tensor(other)\n",
    "            return func(self, other)\n",
    "        return wrapper\n",
    "            \n",
    "    # Basic Operations ===========================================\n",
    "    @make_tensor\n",
    "    def __add__(self,  other: Union[int, float, 'Tensor']) -> 'Tensor':\n",
    "        \"\"\"\n",
    "        a + b\n",
    "        \"\"\"\n",
    "        result = np.add(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"add\")\n",
    "    \n",
    "    def backward_add(self):\n",
    "        \"\"\"\n",
    "        (a + b)' = a' + b'\n",
    "        \"\"\"\n",
    "        self.parents[0].backward(self.grad) \n",
    "        self.parents[1].backward(self.grad)\n",
    "\n",
    "    @make_tensor\n",
    "    def __sub__(self, other: Union[int, float, 'Tensor']) -> 'Tensor':\n",
    "        \"\"\"\n",
    "        a - b\n",
    "        \"\"\"\n",
    "        result = np.subtract(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"sub\")\n",
    "    \n",
    "    def backward_sub(self):\n",
    "        \"\"\"\n",
    "        (a - b)' = a' - b'\n",
    "        The first parent receives the gradient directly, the second parent receives the negation of the gradient.\n",
    "        \"\"\"\n",
    "        self.parents[0].backward(self.grad)\n",
    "        self.parents[1].backward(-self.grad)\n",
    "    \n",
    "    @make_tensor\n",
    "    def __mul__(self, other: Union[int, float, 'Tensor']) -> 'Tensor':\n",
    "        result = np.multiply(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"mul\")\n",
    "    \n",
    "    def backward_mul(self):\n",
    "        \"\"\"\n",
    "        (a * b)' = a' * b + a * b'\n",
    "        The gradient is scaled by the other parent for each respective parent.\n",
    "        \"\"\"\n",
    "        self.parents[0].backward(self.grad * self.parents[1].data) # a' * b\n",
    "        self.parents[1].backward(self.grad * self.parents[0].data) # a * b'\n",
    "    \n",
    "    @make_tensor\n",
    "    def __truediv__(self, other: Union[int, float, 'Tensor']) -> 'Tensor':\n",
    "        result = np.divide(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"div\")\n",
    "    \n",
    "    def backward_div(self):\n",
    "        \"\"\"\n",
    "        (a / b)' = (a' * b - a * b') / b^2\n",
    "        The first parent receives the scaled gradient, and the second parent receives the scaled and negated gradient.\n",
    "        \"\"\"\n",
    "        self.parents[0].backward(self.grad / self.parents[1].data)  # a' / b\n",
    "        self.parents[1].backward(-self.grad * self.parents[0].data / (self.parents[1].data ** 2))  # -a / b^2\n",
    "        \n",
    "    @make_tensor\n",
    "    def __pow__(self, other: Union[int, float, 'Tensor']) -> 'Tensor':\n",
    "        result = np.power(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"pow\")\n",
    "    \n",
    "    def backward_pow(self):\n",
    "        \"\"\"\n",
    "        f(a, b) = a^b\n",
    "        df/da = b * a^(b - 1)\n",
    "        df/db = a^b * ln(a)\n",
    "        \"\"\"\n",
    "        a = self.parents[0].data\n",
    "        b = self.parents[1].data\n",
    "\n",
    "        # find partial derivatives\n",
    "        grad_wrt_a = self.grad * b * (a ** (b - 1))\n",
    "        grad_wrt_b = self.grad * (a ** b) * np.log(a)\n",
    "\n",
    "        # backpropogate\n",
    "        self.parents[0].backward(grad_wrt_a)\n",
    "        self.parents[1].backward(grad_wrt_b)\n",
    "\n",
    "    @make_tensor\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication\n",
    "        \"\"\"\n",
    "        result = np.matmul(self.data, other.data)\n",
    "        return Tensor(result, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"matmul\")\n",
    "\n",
    "    def backward_matmul(self):\n",
    "        \"\"\"\n",
    "        (A @ B)' = A' @ B + A @ B'\n",
    "        \"\"\"\n",
    "        # find partial derivatives\n",
    "        grad_wrt_first_parent = np.matmul(self.grad, self.parents[1].data.T)\n",
    "        grad_wrt_second_parent = np.matmul(self.parents[0].data.T, self.grad)\n",
    "\n",
    "        # backpropogate\n",
    "        self.parents[0].backward(Tensor(grad_wrt_first_parent))\n",
    "        self.parents[1].backward(Tensor(grad_wrt_second_parent))\n",
    "\n",
    "\n",
    "    # Reverse Operations =========================================\n",
    "\n",
    "    # Unary Operations ===========================================\n",
    "\n",
    "    # no decorator because no args to convert to tensors\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    # no decorator because no args to convert to tensors\n",
    "    def __abs__(self):\n",
    "        return Tensor(np.abs(self.data), requires_grad=self.requires_grad, parents=[self], creation_op='abs')\n",
    "\n",
    "    # Reduction Operations =======================================\n",
    "\n",
    "    # Shape Operations ===========================================\n",
    "    def transpose(self):\n",
    "        return Tensor(self.data.transpose(), requires_grad=self.requires_grad, parents=[self], creation_op='transpose')\n",
    "\n",
    "    def backward_transpose(self):\n",
    "        \"\"\"\n",
    "        (A^T)' = (A')\n",
    "        \"\"\"\n",
    "        self.parents[0].backward(self.grad.transpose()) # A'\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.transpose()\n",
    "\n",
    "    # Comparison Operations ======================================\n",
    "\n",
    "    # Indexing, Slicing, Joining, Mutating Ops ==================\n",
    "\n",
    "    # Utils =====================================================\n",
    "\n",
    "    # Other =====================================================\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.data}, requires_grad={self.requires_grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Tensor(2, requires_grad=True)\n",
    "v = Tensor([1, 2, 3], requires_grad=True)\n",
    "m1 = Tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)\n",
    "m2 = Tensor([[1, 2], [3, 4], [5, 6]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[22. 28.]\n",
       " [49. 64.]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = m1 @ m2\n",
    "m4 = m3 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('O') to dtype('float64') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m4\u001b[39m.\u001b[39;49mbackward()\n",
      "Cell \u001b[0;32mIn[55], line 53\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m grad\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grad \u001b[39m# if self is a leaf node, we accumulate gradients\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# time to backpropogate\u001b[39;00m\n\u001b[1;32m     56\u001b[0m backward_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_ops\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op) \u001b[39m# get the correct backward op\u001b[39;00m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('O') to dtype('float64') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "m4.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
