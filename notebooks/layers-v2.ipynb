{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.Tensor import Tensor\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Compute the output of this layer using `input_data`.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "        \"\"\"\n",
    "        Compute the input gradient using `output_gradient` and\n",
    "        chain it with the local gradient.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, input_data):\n",
    "        \"\"\"\n",
    "        A convenient way to chain operations.\n",
    "        \"\"\"\n",
    "        return self.forward(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, lr: float = 0.01):\n",
    "        self.weights = self.init_weights(input_dim, output_dim)\n",
    "        self.bias = self.init_bias(output_dim)\n",
    "        self.lr = lr\n",
    "        self.input = None\n",
    "    \n",
    "    # initialize weights and bias\n",
    "    def init_weights(self, input_dim: int, output_dim: int) -> Tensor:\n",
    "        arr = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        return Tensor(arr, requires_grad=True)\n",
    "    \n",
    "    def init_bias(self, output_dim: int) -> Tensor:\n",
    "        arr = np.zeros((1, output_dim))\n",
    "        return Tensor(arr, requires_grad=True)\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, input_data: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Take an input tensor, multiply it with the weights and add the bias.\n",
    "        \n",
    "        X is a matrix of shape (batch_size, input_dim)\n",
    "        W is a matrix of shape (input_dim, output_dim)\n",
    "        b is a matrix of shape (1, output_dim)\n",
    "\n",
    "        output = X @ W + b, matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.input = input_data # save input for backward pass\n",
    "        # ?not sure if i need to handle the case where batch_size = 1, and input_data is a vector\n",
    "        self.output = input_data @ self.weights.T + self.bias # matrix multiplication\n",
    "        return self.output\n",
    "    \n",
    "    # backward pass\n",
    "        # backward pass\n",
    "    def backward(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store gradients for weights and biases based on the output_gradient\n",
    "        coming from the next layer. Then, compute the gradient for the inputs to be\n",
    "        sent to the previous layer.\n",
    "\n",
    "        output_gradient is a tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.output.grad is None:\n",
    "            raise RuntimeError(\"No gradient found. You might need to call backward on the loss Tensor first.\")\n",
    "        \n",
    "        # compute gradient for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, params: List[Tensor], lr: float = 0.01):\n",
    "        self.params = params # a list of Tensors\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.requires_grad:\n",
    "                param.data -= self.lr * param.grad\n",
    "                param.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            if param.requires_grad:\n",
    "                param.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Dense(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = Tensor(np.random.randn(1), requires_grad=True) # (batch_size, input_dim)\n",
    "input_data = np.array(1)\n",
    "target = Tensor(np.random.randn(1), requires_grad=True) # (batch_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: Tensor([[-0.00544514]], requires_grad=True), shape (1, 1)\n",
      "bias: Tensor([[0.]], requires_grad=True), shape (1, 1)\n",
      "Input data: 1, shape ()\n",
      "Target: Tensor([0.70778891], requires_grad=True), shape (1,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"weights: {layer.weights}, shape {layer.weights.shape}\")\n",
    "print(f\"bias: {layer.bias}, shape {layer.bias.shape}\")\n",
    "print(f\"Input data: {input_data}, shape {input_data.shape}\")\n",
    "print(f\"Target: {target}, shape {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m layer\u001b[39m.\u001b[39;49mweights \u001b[39m@\u001b[39;49m input_data \u001b[39m+\u001b[39m layer\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:135\u001b[0m, in \u001b[0;36mTensor.make_tensor.<locals>.wrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Tensor):\n\u001b[1;32m    134\u001b[0m     other \u001b[39m=\u001b[39m Tensor(other, requires_grad\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad)\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:227\u001b[0m, in \u001b[0;36mTensor.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@make_tensor\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__matmul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    224\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    Matrix multiplication\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m Tensor(result, requires_grad\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad \u001b[39mor\u001b[39;00m other\u001b[39m.\u001b[39mrequires_grad), parents\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m, other], creation_op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatmul\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "layer.weights @ input_data + layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = SGD([layer.weights, layer.bias], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: (2, 1)\n",
      "Bias shape: (1, 1)\n",
      "Input shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights shape:\", layer.weights.shape)\n",
    "print(\"Bias shape:\", layer.bias.shape)\n",
    "print(\"Input shape:\", input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[-0.00108276 -0.00110982]\n",
       " [-0.00262273 -0.00268827]], requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weights @ input_data.T + layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m layer(input_data) \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOutput \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m, shape \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# loss = ((output - target) ** 2).sum() # compute loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# print(f\"Loss {loss}, shape {loss.shape}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 24\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_data):\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    A convenient way to chain operations.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_data)\n",
      "Cell \u001b[0;32mIn[76], line 30\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mTake an input tensor, multiply it with the weights and add the bias.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39moutput = X @ W + b, matrix of shape (batch_size, output_dim)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m input_data \u001b[39m# save input for backward pass\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m input_data \u001b[39m@\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m# matrix multiplication\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:135\u001b[0m, in \u001b[0;36mTensor.make_tensor.<locals>.wrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Tensor):\n\u001b[1;32m    134\u001b[0m     other \u001b[39m=\u001b[39m Tensor(other, requires_grad\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad)\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:227\u001b[0m, in \u001b[0;36mTensor.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@make_tensor\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__matmul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    224\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    Matrix multiplication\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m Tensor(result, requires_grad\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad \u001b[39mor\u001b[39;00m other\u001b[39m.\u001b[39mrequires_grad), parents\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m, other], creation_op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatmul\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": [
    "output = layer(input_data) # forward pass\n",
    "print(f\"Output {output}, shape {output.shape}\")\n",
    "# loss = ((output - target) ** 2).sum() # compute loss\n",
    "# print(f\"Loss {loss}, shape {loss.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad: 1.0\n",
      "Self.grad: 0.0\n",
      "Im in the backward_sum function now\n",
      "self.grad: 1.0\n",
      "self.data: 0.7029554252960425\n",
      "self.grad * np.ones_like(self.data): 1.0\n",
      "Grad: 1.0\n",
      "Self.grad: [[0.]]\n",
      "Grad: [[-1.67684874]]\n",
      "Self.grad: [[0.]]\n",
      "Grad: [[-1.67684874]]\n",
      "Self.grad: [[0.]]\n",
      "Grad: [[-1.67684874]]\n",
      "Self.grad: [0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:123\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    121\u001b[0m backward_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_ops\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op, \u001b[39mNone\u001b[39;00m) \u001b[39m# get the correct backward op\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m backward_op:\n\u001b[0;32m--> 123\u001b[0m     backward_op() \u001b[39m# call the backward op\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBackward op for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op\u001b[39m}\u001b[39;00m\u001b[39m not implemented\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:290\u001b[0m, in \u001b[0;36mTensor.backward_sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mself.data: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mself.grad * np.ones_like(self.data): \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39mnp\u001b[39m.\u001b[39mones_like(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparents[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mones_like(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata))\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:123\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    121\u001b[0m backward_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_ops\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op, \u001b[39mNone\u001b[39;00m) \u001b[39m# get the correct backward op\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m backward_op:\n\u001b[0;32m--> 123\u001b[0m     backward_op() \u001b[39m# call the backward op\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBackward op for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op\u001b[39m}\u001b[39;00m\u001b[39m not implemented\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:219\u001b[0m, in \u001b[0;36mTensor.backward_pow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m grad_wrt_b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39misfinite(grad_wrt_b), grad_wrt_b, \u001b[39m0\u001b[39m) \u001b[39m# replace inf and nan with 0\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# backpropogate\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparents[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward(grad_wrt_a)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbackward(grad_wrt_b)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:123\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    121\u001b[0m backward_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_ops\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op, \u001b[39mNone\u001b[39;00m) \u001b[39m# get the correct backward op\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m backward_op:\n\u001b[0;32m--> 123\u001b[0m     backward_op() \u001b[39m# call the backward op\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBackward op for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op\u001b[39m}\u001b[39;00m\u001b[39m not implemented\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:167\u001b[0m, in \u001b[0;36mTensor.backward_sub\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_sub\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m    (a - b)' = a' - b'\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m    The first parent receives the gradient directly, the second parent receives the negation of the gradient.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparents[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad)\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbackward(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:123\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    121\u001b[0m backward_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_ops\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op, \u001b[39mNone\u001b[39;00m) \u001b[39m# get the correct backward op\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m backward_op:\n\u001b[0;32m--> 123\u001b[0m     backward_op() \u001b[39m# call the backward op\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBackward op for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op\u001b[39m}\u001b[39;00m\u001b[39m not implemented\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:151\u001b[0m, in \u001b[0;36mTensor.backward_add\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_add\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    148\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39m    (a + b)' = a' + b'\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparents[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad) \n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbackward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:123\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    121\u001b[0m backward_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_ops\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op, \u001b[39mNone\u001b[39;00m) \u001b[39m# get the correct backward op\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m backward_op:\n\u001b[0;32m--> 123\u001b[0m     backward_op() \u001b[39m# call the backward op\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBackward op for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreation_op\u001b[39m}\u001b[39;00m\u001b[39m not implemented\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/Tensor.py:236\u001b[0m, in \u001b[0;36mTensor.backward_matmul\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m# find partial derivatives\u001b[39;00m\n\u001b[1;32m    235\u001b[0m grad_wrt_first_parent \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mT)\n\u001b[0;32m--> 236\u001b[0m grad_wrt_second_parent \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparents[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad)\n\u001b[1;32m    238\u001b[0m \u001b[39m# backpropogate\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbackward(grad_wrt_first_parent)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor([[2920.47471859]\n",
       "  [1752.29304344]], requires_grad=True),\n",
       " Tensor([[1130.50583928]], requires_grad=True)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
