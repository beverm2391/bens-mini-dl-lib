{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from lib.Tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(2, 3) * 3\n",
    "\n",
    "a = Tensor(data, requires_grad=True)\n",
    "a = a.clip(2, 8)\n",
    "a.sum().backward()\n",
    "\n",
    "a_pt = torch.tensor(data, requires_grad=True)\n",
    "a_pt = a_pt.clip(2, 8)\n",
    "a_pt.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/m1cj5jqd1cx9xq5bxs_h3g200000gn/T/ipykernel_83925/954677892.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(a_pt.grad)\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(a_pt.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.random.rand(10, 5)\n",
    "y_data = np.random.randint(0, 5, size=(10,1))\n",
    "\n",
    "x = Tensor(x_data, requires_grad=True)\n",
    "y = Tensor(y_data, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0.32585671 0.2932958  0.64263918 0.59614046 0.61774382]\n",
       " [0.04047177 0.64687904 0.49243823 0.14357895 0.26889615]\n",
       " [0.42671793 0.62874981 0.37634977 0.42806211 0.14612539]\n",
       " [0.06158747 0.27286321 0.92372379 0.09474398 0.38038477]\n",
       " [0.8138277  0.28546407 0.69025764 0.14253811 0.9759206 ]\n",
       " [0.21995289 0.65508706 0.76853784 0.35089782 0.6958695 ]\n",
       " [0.52867313 0.70560274 0.13798282 0.76022937 0.58267696]\n",
       " [0.85133483 0.63501001 0.95235131 0.85611409 0.47802595]\n",
       " [0.35250522 0.64806383 0.50537053 0.31774278 0.06645085]\n",
       " [0.61196008 0.81090009 0.29843122 0.43424096 0.15658913]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1e-12\n",
    "x.clip(epsilon, 1 - epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[ -3.36389262  -3.67972091  -1.3265156   -1.55183688  -1.44504433]\n",
       " [ -3.20715059  -0.43559595  -0.70838624  -1.94087023  -1.31343005]\n",
       " [ -1.70326414  -0.92804371  -1.95447267  -1.69697396  -3.84658039]\n",
       " [-11.14918756  -5.19513871  -0.31736871  -9.42630788  -3.86628792]\n",
       " [ -0.61801982  -3.76091734  -1.11207108  -5.84443755  -0.07312215]\n",
       " [ -3.02868381  -0.84597426  -0.52653097  -2.09452043  -0.72518628]\n",
       " [ -0.63738494  -0.3487029   -1.9806261   -0.27413509  -0.54012235]\n",
       " [ -0.48284931  -1.36234353  -0.14646388  -0.46605488  -2.21427081]\n",
       " [ -4.17075941  -1.73506437  -2.72985358  -4.58605242 -10.8451709 ]\n",
       " [ -1.96435293  -0.83844169  -4.83686321  -3.33662272  -7.41651963]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y * x.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lin.NN import Loss\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        self.input = (x, y)\n",
    "        epsilon = 1e-12\n",
    "        x.clip(epsilon, 1. - epsilon) # clip to avoid log(0)\n",
    "        ce = - (y * x.log() + (1. - y) * (1. - x).log()).mean() # cross entropy\n",
    "        return ce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
