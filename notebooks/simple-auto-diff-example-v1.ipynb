{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTensor:\n",
    "    def __init__(self, data, requires_grad=False, parents=None, creation_op=None):\n",
    "        self.data = np.array(data) # hold the data\n",
    "        self.requires_grad = requires_grad # flag to check if gradient is required\n",
    "        self.parents = parents or [] # hold the parents\n",
    "        self.grad = None # hold the gradient (this should only stay None if requires_grad is False)\n",
    "        self.creation_op = creation_op # hold the operation that created the tensor\n",
    "\n",
    "        if self.requires_grad: # if gradient is required, zero initialize the gradient\n",
    "            self.grad = np.zeros_like(self.data)\n",
    "\n",
    "    @property\n",
    "    def backward_ops(self):\n",
    "        \"\"\"\n",
    "        I did this to clearly see what's implemented and what's not\n",
    "        \"\"\"\n",
    "        ops = {\n",
    "            \"add\": self.backward_add,\n",
    "            \"mul\": self.backward_mul\n",
    "        }\n",
    "        return ops\n",
    "    \n",
    "    def make_tensor(func):\n",
    "        \"\"\"\n",
    "        Decorator to convert the 'other' arg to a tensor if its not already a tensor\n",
    "        \"\"\"\n",
    "        @wraps(func) # does this line matte?\n",
    "        def wrapper(self, other):\n",
    "            if not isinstance(other, SimpleTensor):\n",
    "                other = SimpleTensor(other)\n",
    "            return func(self, other)\n",
    "        return wrapper\n",
    "\n",
    "    @make_tensor\n",
    "    def __add__(self, other):\n",
    "        data = np.add(self.data, other.data)\n",
    "        return SimpleTensor(data, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"add\")\n",
    "    \n",
    "    def backward_add(self):\n",
    "        self.parents[0].backward(self.grad)\n",
    "        self.parents[1].backward(self.grad)\n",
    "    \n",
    "    @make_tensor\n",
    "    def __mul__(self, other):\n",
    "        data = np.multiply(self.data, other.data)\n",
    "        return SimpleTensor(data, requires_grad=(self.requires_grad or other.requires_grad), parents=[self, other], creation_op=\"mul\")\n",
    "    \n",
    "    def backward_mul(self):\n",
    "        grad_wrt_first_parent = self.grad * self.parents[1].data\n",
    "        grad_wrt_second_parent = self.grad * self.parents[0].data\n",
    "        self.parents[0].backward(grad_wrt_first_parent)\n",
    "        self.parents[1].backward(grad_wrt_second_parent)\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        if not self.requires_grad: # if gradient is not required, return\n",
    "            return\n",
    "        \n",
    "        if grad is None:  # if we call backward without passing a gradient, initialize the gradient to 1\n",
    "            grad = np.ones_like(self.data)\n",
    "\n",
    "        if self.grad is None:  # initialize self.grad if it's None\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad  # accumulate gradient\n",
    "\n",
    "        if self.creation_op is None: # if the tensor was created by the user\n",
    "            return\n",
    "        \n",
    "        # run the appropriate backward operation\n",
    "        backward_op = self.backward_ops.get(self.creation_op, None)\n",
    "        if backward_op is not None:\n",
    "            backward_op() # call it\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only addition and multiplication implemented\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"SimpleTensor(data={self.data}, requires_grad={self.requires_grad}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 12]\n",
      " [14 16]]\n",
      "[[2 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "m1 = SimpleTensor([[1, 2], [3, 4]], requires_grad=True)\n",
    "m2 = SimpleTensor([[5, 6], [7, 8]], requires_grad=True)\n",
    "v = SimpleTensor([9, 10], requires_grad=True)\n",
    "\n",
    "# multiplying two tensors\n",
    "m3 = m1 * m2 * 2\n",
    "\n",
    "m3.backward()\n",
    "\n",
    "print(m1.grad)\n",
    "print(m2.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
