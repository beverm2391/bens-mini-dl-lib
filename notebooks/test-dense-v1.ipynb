{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from lib.TensorV2 import Tensor, force_tensor_method\n",
    "from lib.NN import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__() # init Layer\n",
    "        self.weights = self._init_weights(input_dim, output_dim)\n",
    "        self.biases = self._init_biases(output_dim)\n",
    "\n",
    "    def _init_weights(self, input_dim: int, output_dim: int) -> Tensor:\n",
    "        assert input_dim > 0 and output_dim > 0\n",
    "        arr = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        return Tensor(arr, requires_grad=True) # defaulting this to True for sanity\n",
    "    \n",
    "    def _init_biases(self, output_dim: int) -> Tensor:\n",
    "        assert output_dim > 0\n",
    "        arr = np.zeros((1, output_dim))\n",
    "        return Tensor(arr, requires_grad=True) # defaulting this to True for sanity\n",
    "    \n",
    "    def parameters(self) -> List[Tensor]:\n",
    "        return [self.weights, self.biases]\n",
    "\n",
    "    @force_tensor_method\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # input_features = x.shape[1] # input_dim\n",
    "        # if input_features != self.weights.shape[0]: # input_dim != weights.shape[0] - make sure the tensor matches the way the weights were initialized\n",
    "        #     raise RuntimeError(f\"Input tensor with {input_features} features should match layer input dim {self.weights.shape[0]}\")\n",
    "\n",
    "        #? not sure if i need to handle the case where batch_size = 1, and x is a vector\n",
    "        # xW or Wx? any transposition?\n",
    "        # https://stackoverflow.com/questions/63006388/should-i-transpose-features-or-weights-in-neural-network\n",
    "        # \"Should I transpose features or weights in Neural network?\" - in torch convention, you should transpose weights, but use matmul with the features first.\n",
    "        return x @ self.weights.T + self.biases # matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (1,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m t \u001b[39m=\u001b[39m Tensor(t_data, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m dense \u001b[39m=\u001b[39m Dense(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m dense(t)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/NN.py:118\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/TensorV2.py:406\u001b[0m, in \u001b[0;36mforce_tensor_method.<locals>.wrapper\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, Tensor):\n\u001b[1;32m    405\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput data to layer \u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m need to be a Tensor, is \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 406\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[30], line 30\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@force_tensor_method\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     22\u001b[0m     \u001b[39m# input_features = x.shape[1] # input_dim\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# https://stackoverflow.com/questions/63006388/should-i-transpose-features-or-weights-in-neural-network\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39m# \"Should I transpose features or weights in Neural network?\" - in torch convention, you should transpose weights, but use matmul with the features first.\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m@\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\u001b[39m.\u001b[39;49mT \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbiases\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/TensorV2.py:56\u001b[0m, in \u001b[0;36mTensor.make_tensor.<locals>.wrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Tensor):\n\u001b[1;32m     55\u001b[0m     other \u001b[39m=\u001b[39m Tensor(other, requires_grad\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/Documents/Coding/bens-mini-dl/lib/TensorV2.py:107\u001b[0m, in \u001b[0;36mTensor.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mAdd two tensors\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m rg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad \u001b[39mor\u001b[39;00m other\u001b[39m.\u001b[39mrequires_grad\n\u001b[0;32m--> 107\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49madd(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m    108\u001b[0m out \u001b[39m=\u001b[39m Tensor(out, (\u001b[39mself\u001b[39m, other), \u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m, requires_grad\u001b[39m=\u001b[39mrg)\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m():\n\u001b[1;32m    111\u001b[0m     \u001b[39m# ? debug broadcasting issues\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[39m# print(f\"Self data shape: {self.data.shape}\")\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39m# print(f\"Other data shape: {other.data.shape}\")\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39m# print(f\"Self grad shape: {self.grad.shape}\")\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39m# print(f\"Out grad shape: {out.grad.shape}\")\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (1,3) "
     ]
    }
   ],
   "source": [
    "t_data = np.random.rand(2, 3) # 2 samples, 3 features\n",
    "\n",
    "t = Tensor(t_data, requires_grad=True)\n",
    "\n",
    "dense = Dense(2, 3)\n",
    "dense(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 0.0102087   0.01122759]\n",
      " [ 0.03183444 -0.01612629]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "t = Tensor(np.random.randn(*shape), requires_grad=True)\n",
    "\n",
    "dense = Dense(*shape[::-1])\n",
    "\n",
    "out = dense(t)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
